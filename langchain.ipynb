{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4501b296-4a9d-4100-a3c2-f267526a714a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.3\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "531b7b9a-6398-4b00-88e8-d83db22dd8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import config\n",
    "\n",
    "# load deps\n",
    "from KMOpenAI import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d09b909-4464-431f-bced-3842ae5696ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiktok = Tokenizer()\n",
    "\n",
    "def count_tokens(query):\n",
    "    return tiktok.get_token_count(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4fba81f-9a5c-4e9f-9698-ae1dd4f04430",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = config.openai['key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6c99883-a6c2-4216-a8cc-50480250b459",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"messages\": [{\"role\": \"user\", \"content\": \"hello\"}], \"temperature\": 0.0, \"top_p\": 1.0, \"frequency_penalty\": 0.0, \"presence_penalty\": 0.0, \"max_tokens\": 1000, \"model\": \"gpt-3.5-turbo\", \"stream\": false}'\n",
      "{\n",
      "  \"id\": \"chatcmpl-8xe7XbicWJKYsT7VTAn9BUB536eXG\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1709226267,\n",
      "  \"model\": \"gpt-3.5-turbo-0125\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Hello! How can I assist you today?\"\n",
      "      },\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 8,\n",
      "    \"completion_tokens\": 9,\n",
      "    \"total_tokens\": 17\n",
      "  },\n",
      "  \"system_fingerprint\": \"fp_86156a94a0\"\n",
      "}\n",
      "\n",
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAIAPI(os.environ['OPENAI_API_KEY'])\n",
    "result = llm.openai_completions(\"hello\")\n",
    "print(result['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ac64917-22fc-48c2-9e52-8519721fb5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain_openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ce97dbd-23b4-4ca7-aa02-2f41a6fc09a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n2+2 is equal to 4.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = OpenAI()\n",
    "llm.invoke(\"hello, what is 2+2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "d2236ce8-c85a-4357-abcf-6f25392e999c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hello! Who is the president of USA?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"AI\",\n",
    "        \"content\": \"The president of USA is Joe Biden.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Nice. What's his height?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"AI\",\n",
    "        \"content\": \"It is 6 feet.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Add 2 to this and convert it into cm.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"AI\",\n",
    "        \"content\": \"It is now 243.84 centimeters.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"He belongs to what party?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"AI\",\n",
    "        \"content\": \"He belongs to Democratic Party of USA.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What was the previous president Barack Obama's height?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"AI\",\n",
    "        \"content\": \"Barack Obama's height was 6.5 feet.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Can you add 100 cms to my frist question person's height?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"AI\",\n",
    "        \"content\": \"Sure, Joe Biden's new height is 182.88 cms (6 feet) + 100 cms = 282.88 cms or 9.3 feet.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Thank you.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc28c68b-fbdc-4c55-b414-885f7a7580da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaaf17f-4fe5-4e64-ac96-411aef34201c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cc2c3b-1ce3-47be-b0e8-e58e397afb9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15f12c8-de35-4c27-bfe9-8a3dc7398d38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362aa071-2416-47a0-a79d-31031d9dd46b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a3bec28c-bcb8-48a8-b643-ffdf654ac832",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKEN_LIMIT = 200\n",
    "MAX_SUMMARY_TOKEN_LIMIT = MAX_TOKEN_LIMIT - 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3b23aa27-495f-4384-b6d6-183a1bf963f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history_prompt_template = \\\n",
    "\"\"\"\n",
    "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary within provided length of tokens.\n",
    "\n",
    "EXAMPLE\n",
    "Current summary:\n",
    "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
    "\n",
    "New lines of conversation:\n",
    "Human: Why do you think artificial intelligence is a force for good?\n",
    "AI: Because artificial intelligence will help humans reach their full potential.\n",
    "\n",
    "New summary length:\n",
    "20\n",
    "\n",
    "New summary:\n",
    "Human queries AI on AI's view. AI deems AI beneficial, enabling human potential...\n",
    "END OF EXAMPLE\n",
    "\n",
    "Current summary:\n",
    "{summary}\n",
    "\n",
    "New lines of conversation:\n",
    "{new_lines}\n",
    "\n",
    "New summary length:\n",
    "{length_of_summary}\n",
    "\n",
    "New summary:\n",
    "\"\"\"\n",
    "chat_history_prompt_template_keywords = [\"{summary}\", \"{new_lines}\", \"{length_of_summary}\"]\n",
    "\n",
    "\n",
    "\n",
    "qna_context_prompt_template = \\\n",
    "\"\"\"\n",
    "You are a helpful AI assistant for Morgan Stanley users. Using conversational context provide answers to your best of capability.\n",
    "\n",
    "EXAMPLE\n",
    "Context:\n",
    "The human asks what the is Barack Obama's height? The AI says its 6 feet.\n",
    "\n",
    "Question:\n",
    "Add 2 feet to his height.\n",
    "\n",
    "Response:\n",
    "Barack Obama's new height will be 8 feet.\n",
    "END OF EXAMPLE\n",
    "\n",
    "Context:\n",
    "{chat_history}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Response:\n",
    "\"\"\"\n",
    "qna_context_prompt_template_keywords = [\"{chat_history}\", \"{query}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a9ca81bd-8eac-4714-934e-f87727bbcb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAIAPI(os.environ['OPENAI_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "60668635-c4b6-484f-a6fa-d377317d630a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationRollingWindow():\n",
    "    \n",
    "    def __init__(self, max_token_limit, max_summary_token_limit):\n",
    "        self.prompt_template = chat_history_prompt_template\n",
    "        self.chat_history_prompt_template_keywords = chat_history_prompt_template_keywords        \n",
    "        prompt_template_len = self.get_prompt_template_len()\n",
    "        self.max_token_limit  = max_token_limit - prompt_template_len\n",
    "        self.summary_len = max_summary_token_limit\n",
    "        self.line_delimitter = \"\\n\\n\"\n",
    "        \n",
    "        self.new_lines = \"\"\n",
    "        self.current_summary = \"\"\n",
    "\n",
    "        self.conversation = []\n",
    "        \n",
    "\n",
    "    def get_prompt_template_len(self):\n",
    "        template = chat_history_prompt_template\n",
    "        for key in self.chat_history_prompt_template_keywords:\n",
    "            template = template.replace(key, \"\")\n",
    "        return count_tokens(template)\n",
    "        \n",
    "    def count_tokens(self, text):\n",
    "        return tiktok.get_token_count(text)\n",
    "\n",
    "    def extract_content(self, messages):\n",
    "        try:\n",
    "            if isinstance(messages, list):\n",
    "                user_content = \"\"\n",
    "                ai_content = \"\"\n",
    "                for message in messages:\n",
    "                    if message[\"role\"] == \"user\":\n",
    "                        user_content += message[\"content\"] + \"\\n\"\n",
    "                    elif message[\"role\"] == \"AI\":\n",
    "                        ai_content += message[\"content\"]\n",
    "                return f\"User: {user_content}AI: {ai_content}\"\n",
    "            else:\n",
    "                return f\"ERROR: pass a list of dict\"\n",
    "        except Exception as e:\n",
    "            return f\"ERROR: {str(e)}\"\n",
    "\n",
    "    def get_hat_history_context(self, conversation):\n",
    "        if not isinstance(conversation, list):\n",
    "            return \"\"\n",
    "            \n",
    "        # New message\n",
    "        new_message_str = self.extract_content(conversation)\n",
    "        count_new_message = self.count_tokens(new_message_str)\n",
    "        print(f\"<NEW_MESSAGE>\\n{new_message_str} | Count: {count_new_message}\\n\")\n",
    "        \n",
    "        # New lines\n",
    "        line_breaker = \"\"\n",
    "        if len(self.new_lines) > 0:\n",
    "            line_breaker = self.line_delimitter\n",
    "        self.new_lines += line_breaker + new_message_str\n",
    "\n",
    "        # Entire chat history\n",
    "        conversation_history = (self.current_summary + \"\\n\\n\" + self.new_lines).strip(\"\\n\").strip()\n",
    "        count_total_tokens = self.count_tokens(conversation_history)\n",
    "        print(f\"\\n<CONVERSATION>\\n{conversation_history} | Count: {count_total_tokens} | Limit: {self.max_token_limit}\\n\")\n",
    "\n",
    "        if count_total_tokens >= self.max_token_limit:\n",
    "            new_summary = self.chat_summarizer(self.new_lines)\n",
    "            self.current_summary = new_summary.strip(\"\\n\").strip()\n",
    "            self.new_lines = \"\"\n",
    "        \n",
    "        return f\"{self.current_summary}\\n\\n{self.new_lines}\".strip(\"\\n\").strip()\n",
    "\n",
    "    def invoke_openai(self, prompt):\n",
    "        result = llm.openai_completions(prompt)\n",
    "        token_count = self.count_tokens(prompt)\n",
    "        print(f\"OpenAI Completions called: {token_count}\")\n",
    "        return result['choices'][0]['message']['content']\n",
    "        \n",
    "    def chat_summarizer(self, new_lines, summary_len=None):\n",
    "        if not summary_len:\n",
    "            summary_len = str(self.summary_len)\n",
    "        summary_prompt = self.prompt_template \\\n",
    "                            .replace('{summary}', self.current_summary) \\\n",
    "                            .replace('{new_lines}', new_lines) \\\n",
    "                            .replace('{length_of_summary}', summary_len)\n",
    "        print(\"\\n+++++\\n\", summary_prompt, \"\\n++++\\n\")\n",
    "        return self.invoke_openai(summary_prompt)\n",
    "\n",
    "\n",
    "    def chat_qna(self, user_query):\n",
    "        \n",
    "        query = {\"role\": \"user\", \"content\": user_query}\n",
    "        \n",
    "        if len(self.conversation) == 0:\n",
    "            prompt = qna_context_prompt_template.replace(\"{chat_history}\", \"\").replace(\"{query}\", user_query)\n",
    "            result = self.invoke_openai(prompt)\n",
    "            response = {\"role\": \"AI\", \"content\": result}\n",
    "            self.conversation = [query, response]\n",
    "        else:\n",
    "            chat_history = self.get_hat_history_context(self.conversation)\n",
    "            print(\"\\n\\n CHAT HISTORY RETRIEVED \\n\\n\", chat_history)\n",
    "            prompt = qna_context_prompt_template.replace(\"{chat_history}\", chat_history).replace(\"{query}\", user_query)\n",
    "            result = self.invoke_openai(prompt)\n",
    "            response = {\"role\": \"AI\", \"content\": result}\n",
    "\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a41a84d4-b5d6-4a5d-9434-df953715823c",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_qna = ConversationRollingWindow(max_token_limit=MAX_TOKEN_LIMIT, max_summary_token_limit=MAX_SUMMARY_TOKEN_LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "938780b5-a3a1-43ec-ace6-ce0bc3e14670",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"messages\": [{\"role\": \"user\", \"content\": \"\\\\nYou are a helpful AI assistant for Morgan Stanley users. Using conversational context provide answers to your best of capability.\\\\n\\\\nEXAMPLE\\\\nContext:\\\\nThe human asks what the is Barack Obama\\'s height? The AI says its 6 feet.\\\\n\\\\nQuestion:\\\\nAdd 2 feet to his height.\\\\n\\\\nResponse:\\\\nBarack Obama\\'s new height will be 8 feet.\\\\nEND OF EXAMPLE\\\\n\\\\nContext:\\\\n\\\\n\\\\nQuestion:\\\\nhi\\\\n\\\\nResponse:\\\\n\"}], \"temperature\": 0.0, \"top_p\": 1.0, \"frequency_penalty\": 0.0, \"presence_penalty\": 0.0, \"max_tokens\": 1000, \"model\": \"gpt-3.5-turbo\", \"stream\": false}'\n",
      "{\n",
      "  \"id\": \"chatcmpl-8xhJ4faleXgpGSH1qPTDWk7lpValu\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1709238514,\n",
      "  \"model\": \"gpt-3.5-turbo-0125\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Hello! How can I assist you today?\"\n",
      "      },\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 90,\n",
      "    \"completion_tokens\": 9,\n",
      "    \"total_tokens\": 99\n",
      "  },\n",
      "  \"system_fingerprint\": \"fp_86156a94a0\"\n",
      "}\n",
      "\n",
      "OpenAI Completions called: 83\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'role': 'AI', 'content': 'Hello! How can I assist you today?'}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_qna.chat_qna('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1ef10727-44ab-4179-acb5-88886d7ea699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NEW_MESSAGE>\n",
      "User: hi\n",
      "AI: Hello! How can I assist you today? | Count: 15\n",
      "\n",
      "\n",
      "<CONVERSATION>\n",
      "User: hi\n",
      "AI: Hello! How can I assist you today? | Count: 15 | Limit: 70\n",
      "\n",
      "b'{\"messages\": [{\"role\": \"user\", \"content\": \"\\\\nYou are a helpful AI assistant for Morgan Stanley users. Using conversational context provide answers to your best of capability.\\\\n\\\\nEXAMPLE\\\\nContext:\\\\nThe human asks what the is Barack Obama\\'s height? The AI says its 6 feet.\\\\n\\\\nQuestion:\\\\nAdd 2 feet to his height.\\\\n\\\\nResponse:\\\\nBarack Obama\\'s new height will be 8 feet.\\\\nEND OF EXAMPLE\\\\n\\\\nContext:\\\\nUser: hi\\\\nAI: Hello! How can I assist you today?\\\\n\\\\nQuestion:\\\\nwhats ur name?\\\\n\\\\nResponse:\\\\n\"}], \"temperature\": 0.0, \"top_p\": 1.0, \"frequency_penalty\": 0.0, \"presence_penalty\": 0.0, \"max_tokens\": 1000, \"model\": \"gpt-3.5-turbo\", \"stream\": false}'\n",
      "{\n",
      "    \"error\": {\n",
      "        \"message\": \"Rate limit reached for gpt-3.5-turbo in organization org-A0JANTlqWEhEWrdT8VHDvUf2 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\",\n",
      "        \"type\": \"requests\",\n",
      "        \"param\": null,\n",
      "        \"code\": \"rate_limit_exceeded\"\n",
      "    }\n",
      "}\n",
      "\n",
      "OpenAI Completions called: 101\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'choices'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcontext_qna\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_qna\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwhats ur name?\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[67], line 98\u001b[0m, in \u001b[0;36mConversationRollingWindow.chat_qna\u001b[0;34m(self, user_query)\u001b[0m\n\u001b[1;32m     96\u001b[0m     chat_history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_hat_history_context(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconversation)\n\u001b[1;32m     97\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m qna_context_prompt_template\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{chat_history}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, chat_history)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{query}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, user_query)\n\u001b[0;32m---> 98\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke_openai\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m     response \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: result}\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "Cell \u001b[0;32mIn[67], line 73\u001b[0m, in \u001b[0;36mConversationRollingWindow.invoke_openai\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m     71\u001b[0m token_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcount_tokens(prompt)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpenAI Completions called: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoken_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchoices\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'choices'"
     ]
    }
   ],
   "source": [
    "context_qna.chat_qna('whats ur name?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32c802c-d0a4-4106-aad0-22ed5dc355b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be244e18-68e5-4b0f-bf45-416440c62dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc07c2c-0e11-4253-88cd-a084d68f2543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e96f14d-1237-4db4-a2f6-3d9a0771a629",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4c564c-533f-4e9a-b178-2a7420ca8f85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4621eb4e-ca84-46b4-899f-dee69e414e92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f7228a60-edc9-4098-a54e-335ec79856a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_qna = ConversationRollingWindow(max_token_limit=MAX_TOKEN_LIMIT, max_summary_token_limit=MAX_SUMMARY_TOKEN_LIMIT)\n",
    "\n",
    "def qna(user_query):\n",
    "    query = {\"role\": \"user\", \"content\": user_query}\n",
    "    chat_history = context.get_hat_history_context(query)\n",
    "    prompt = qna_context_prompt_template.replace(\"{chat_history}\", chat_history).replace(\"{query}\", user_query)\n",
    "    result = context.invoke_openai(prompt)\n",
    "    response = {\"role\": \"AI\", \"content\": result}\n",
    "    return [query, response]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7e472f99-fbdf-455c-a9b2-affe4b85ee51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'messages' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mqna\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHello\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[37], line 5\u001b[0m, in \u001b[0;36mqna\u001b[0;34m(user_query)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mqna\u001b[39m(user_query):\n\u001b[1;32m      4\u001b[0m     query \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: user_query}\n\u001b[0;32m----> 5\u001b[0m     chat_history \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_hat_history_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m qna_prompt\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{chat_history}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, chat_history)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{query}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, user_query)\n\u001b[1;32m      7\u001b[0m     result \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39minvoke_openai(prompt)\n",
      "Cell \u001b[0;32mIn[35], line 40\u001b[0m, in \u001b[0;36mConversationRollingWindow.get_hat_history_context\u001b[0;34m(self, conversation)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_hat_history_context\u001b[39m(\u001b[38;5;28mself\u001b[39m, conversation):\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[43mmessages\u001b[49m, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# New message\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'messages' is not defined"
     ]
    }
   ],
   "source": [
    "res = qna(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7ccd2223-0a98-477c-92fb-7f41d5b3c0ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'Hello'},\n",
       " {'role': 'AI', 'content': 'Hello! How can I assist you today?'}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9b9c57-1dd4-4231-8e99-4568cc5ceea1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f882523-be6c-4cbc-a1ed-a0525feae92f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b52913f-15f0-43e0-80c2-bfc78ffd5d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "a9a88f59-f601-4c2f-ad16-ef95b19aee85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NEW_MESSAGE>\n",
      "ERROR: pass a dict or list | Count: 7\n",
      "\n",
      "\n",
      "<CONVERSATION>\n",
      "ERROR: pass a dict or list | Count: 7 | Limit: 70\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat_history = context.get_hat_history_context(user_query)\n",
    "template = qna_prompt.replace(\"{chat_history}\", chat_history).replace(\"{query}\", user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2083f87e-6226-45e2-81e2-bc390ec19c7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33539a2a-446c-454f-9e60-64828cebeffd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8129ce55-62ea-405f-8479-cc86bc840c93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380988ca-cae6-4f3f-89be-1f5abba8da6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb8478d-9dd4-42ce-921e-749f467d59c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "eaf89f91-5c3e-463d-a02d-4f6d3255b5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NEW_MESSAGE>\n",
      "User: Hello! Who is the president of USA?\n",
      "AI: The president of USA is Joe Biden. | Count: 21\n",
      "\n",
      "\n",
      "<CONVERSATION>\n",
      "User: Hello! Who is the president of USA?\n",
      "AI: The president of USA is Joe Biden. | Count: 21 | Limit: 70\n",
      "\n",
      "\n",
      "\n",
      "=====OUTPUT=====\n",
      "\n",
      " User: Hello! Who is the president of USA?\n",
      "AI: The president of USA is Joe Biden. \n",
      "=============\n",
      "\n"
     ]
    }
   ],
   "source": [
    "convo = sum_chat.handle_conversation(chat_history[:2])\n",
    "print(\"\\n\\n=====OUTPUT=====\\n\\n\", convo, \"\\n=============\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "5cb0cdfe-4075-4510-ba08-99fa6a9ca082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NEW_MESSAGE>\n",
      "User: Nice. What's his height?\n",
      "AI: It is 6 feet. | Count: 17\n",
      "\n",
      "\n",
      "<CONVERSATION>\n",
      "User: Hello! Who is the president of USA?\n",
      "AI: The president of USA is Joe Biden.\n",
      "\n",
      "User: Nice. What's his height?\n",
      "AI: It is 6 feet. | Count: 38 | Limit: 70\n",
      "\n",
      "\n",
      "\n",
      "=====OUTPUT=====\n",
      "\n",
      " User: Hello! Who is the president of USA?\n",
      "AI: The president of USA is Joe Biden.\n",
      "\n",
      "User: Nice. What's his height?\n",
      "AI: It is 6 feet. \n",
      "=============\n",
      "\n"
     ]
    }
   ],
   "source": [
    "convo = sum_chat.handle_conversation(chat_history[2:4])\n",
    "print(\"\\n\\n=====OUTPUT=====\\n\\n\", convo, \"\\n=============\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "8c7b68f6-b91a-47ee-a8f2-52ef56294fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NEW_MESSAGE>\n",
      "User: Add 2 to this and convert it into cm.\n",
      "AI: It is now 243.84 centimeters. | Count: 25\n",
      "\n",
      "\n",
      "<CONVERSATION>\n",
      "User: Hello! Who is the president of USA?\n",
      "AI: The president of USA is Joe Biden.\n",
      "\n",
      "User: Nice. What's his height?\n",
      "AI: It is 6 feet.\n",
      "\n",
      "User: Add 2 to this and convert it into cm.\n",
      "AI: It is now 243.84 centimeters. | Count: 63 | Limit: 70\n",
      "\n",
      "\n",
      "\n",
      "=====OUTPUT=====\n",
      "\n",
      " User: Hello! Who is the president of USA?\n",
      "AI: The president of USA is Joe Biden.\n",
      "\n",
      "User: Nice. What's his height?\n",
      "AI: It is 6 feet.\n",
      "\n",
      "User: Add 2 to this and convert it into cm.\n",
      "AI: It is now 243.84 centimeters. \n",
      "=============\n",
      "\n"
     ]
    }
   ],
   "source": [
    "convo = sum_chat.handle_conversation(chat_history[4:6])\n",
    "print(\"\\n\\n=====OUTPUT=====\\n\\n\", convo, \"\\n=============\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "8bd04f55-63d3-443f-a785-5a979998561e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NEW_MESSAGE>\n",
      "User: He belongs to what party?\n",
      "AI: He belongs to Democratic Party of USA. | Count: 18\n",
      "\n",
      "\n",
      "<CONVERSATION>\n",
      "User: Hello! Who is the president of USA?\n",
      "AI: The president of USA is Joe Biden.\n",
      "\n",
      "User: Nice. What's his height?\n",
      "AI: It is 6 feet.\n",
      "\n",
      "User: Add 2 to this and convert it into cm.\n",
      "AI: It is now 243.84 centimeters.\n",
      "\n",
      "User: He belongs to what party?\n",
      "AI: He belongs to Democratic Party of USA. | Count: 81 | Limit: 70\n",
      "\n",
      "\n",
      "+++++\n",
      " \n",
      "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary within provided length of tokens.\n",
      "\n",
      "EXAMPLE\n",
      "Current summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Why do you think artificial intelligence is a force for good?\n",
      "AI: Because artificial intelligence will help humans reach their full potential.\n",
      "\n",
      "New summary length:\n",
      "20\n",
      "\n",
      "New summary:\n",
      "Human queries AI on AI's view. AI deems AI beneficial, enabling human potential...\n",
      "END OF EXAMPLE\n",
      "\n",
      "Current summary:\n",
      "\n",
      "\n",
      "New lines of conversation:\n",
      "User: Hello! Who is the president of USA?\n",
      "AI: The president of USA is Joe Biden.\n",
      "\n",
      "User: Nice. What's his height?\n",
      "AI: It is 6 feet.\n",
      "\n",
      "User: Add 2 to this and convert it into cm.\n",
      "AI: It is now 243.84 centimeters.\n",
      "\n",
      "User: He belongs to what party?\n",
      "AI: He belongs to Democratic Party of USA.\n",
      "\n",
      "New summary length:\n",
      "50\n",
      "\n",
      "New summary:\n",
      " \n",
      "++++\n",
      "\n",
      "b'{\"messages\": [{\"role\": \"user\", \"content\": \"\\\\nProgressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary within provided length of tokens.\\\\n\\\\nEXAMPLE\\\\nCurrent summary:\\\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\\\\n\\\\nNew lines of conversation:\\\\nHuman: Why do you think artificial intelligence is a force for good?\\\\nAI: Because artificial intelligence will help humans reach their full potential.\\\\n\\\\nNew summary length:\\\\n20\\\\n\\\\nNew summary:\\\\nHuman queries AI on AI\\'s view. AI deems AI beneficial, enabling human potential...\\\\nEND OF EXAMPLE\\\\n\\\\nCurrent summary:\\\\n\\\\n\\\\nNew lines of conversation:\\\\nUser: Hello! Who is the president of USA?\\\\nAI: The president of USA is Joe Biden.\\\\n\\\\nUser: Nice. What\\'s his height?\\\\nAI: It is 6 feet.\\\\n\\\\nUser: Add 2 to this and convert it into cm.\\\\nAI: It is now 243.84 centimeters.\\\\n\\\\nUser: He belongs to what party?\\\\nAI: He belongs to Democratic Party of USA.\\\\n\\\\nNew summary length:\\\\n50\\\\n\\\\nNew summary:\\\\n\"}], \"temperature\": 0.0, \"top_p\": 1.0, \"frequency_penalty\": 0.0, \"presence_penalty\": 0.0, \"max_tokens\": 1000, \"model\": \"gpt-3.5-turbo\", \"stream\": false}'\n",
      "{\n",
      "  \"id\": \"chatcmpl-8xghNWVaRv8HDDmKrz8oDdpowsXxx\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1709236177,\n",
      "  \"model\": \"gpt-3.5-turbo-0125\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"User inquires about USA president. AI responds with Joe Biden. User asks about his height, AI states 6 feet. User requests conversion to cm after adding 2, AI provides 243.84 cm. User queries Biden's party, AI confirms Democratic Party.\"\n",
      "      },\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 220,\n",
      "    \"completion_tokens\": 54,\n",
      "    \"total_tokens\": 274\n",
      "  },\n",
      "  \"system_fingerprint\": \"fp_86156a94a0\"\n",
      "}\n",
      "\n",
      "OpenAI Completions called: 213\n",
      "\n",
      "\n",
      "=====OUTPUT=====\n",
      "\n",
      " User: Hello! Who is the president of USA?\n",
      "AI: The president of USA is Joe Biden.\n",
      "\n",
      "User: Nice. What's his height?\n",
      "AI: It is 6 feet.\n",
      "\n",
      "User: Add 2 to this and convert it into cm.\n",
      "AI: It is now 243.84 centimeters.\n",
      "\n",
      "User: He belongs to what party?\n",
      "AI: He belongs to Democratic Party of USA. \n",
      "=============\n",
      "\n"
     ]
    }
   ],
   "source": [
    "convo = sum_chat.handle_conversation(chat_history[6:8])\n",
    "print(\"\\n\\n=====OUTPUT=====\\n\\n\", convo, \"\\n=============\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "15f51eb7-75aa-4ce8-b3a5-f711d98c450f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NEW_MESSAGE>\n",
      "User: What was the previous president Barack Obama's height?\n",
      "AI: Barack Obama's height was 6.5 feet. | Count: 25\n",
      "\n",
      "\n",
      "<CONVERSATION>\n",
      "User inquires about USA president. AI responds with Joe Biden. User asks about his height, AI states 6 feet. User requests conversion to cm, AI provides 243.84 cm. User queries his political party, AI confirms Democratic Party.\n",
      "\n",
      "User: What was the previous president Barack Obama's height?\n",
      "AI: Barack Obama's height was 6.5 feet. | Count: 75 | Limit: 70\n",
      "\n",
      "\n",
      "+++++\n",
      " \n",
      "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary within provided length of tokens.\n",
      "\n",
      "EXAMPLE\n",
      "Current summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Why do you think artificial intelligence is a force for good?\n",
      "AI: Because artificial intelligence will help humans reach their full potential.\n",
      "\n",
      "New summary length:\n",
      "20\n",
      "\n",
      "New summary:\n",
      "Human queries AI on AI's view. AI deems AI beneficial, enabling human potential...\n",
      "END OF EXAMPLE\n",
      "\n",
      "Current summary:\n",
      "User inquires about USA president. AI responds with Joe Biden. User asks about his height, AI states 6 feet. User requests conversion to cm, AI provides 243.84 cm. User queries his political party, AI confirms Democratic Party.\n",
      "\n",
      "New lines of conversation:\n",
      "User: What was the previous president Barack Obama's height?\n",
      "AI: Barack Obama's height was 6.5 feet.\n",
      "\n",
      "New summary length:\n",
      "50\n",
      "\n",
      "New summary:\n",
      " \n",
      "++++\n",
      "\n",
      "b'{\"messages\": [{\"role\": \"user\", \"content\": \"\\\\nProgressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary within provided length of tokens.\\\\n\\\\nEXAMPLE\\\\nCurrent summary:\\\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\\\\n\\\\nNew lines of conversation:\\\\nHuman: Why do you think artificial intelligence is a force for good?\\\\nAI: Because artificial intelligence will help humans reach their full potential.\\\\n\\\\nNew summary length:\\\\n20\\\\n\\\\nNew summary:\\\\nHuman queries AI on AI\\'s view. AI deems AI beneficial, enabling human potential...\\\\nEND OF EXAMPLE\\\\n\\\\nCurrent summary:\\\\nUser inquires about USA president. AI responds with Joe Biden. User asks about his height, AI states 6 feet. User requests conversion to cm, AI provides 243.84 cm. User queries his political party, AI confirms Democratic Party.\\\\n\\\\nNew lines of conversation:\\\\nUser: What was the previous president Barack Obama\\'s height?\\\\nAI: Barack Obama\\'s height was 6.5 feet.\\\\n\\\\nNew summary length:\\\\n50\\\\n\\\\nNew summary:\\\\n\"}], \"temperature\": 0.0, \"top_p\": 1.0, \"frequency_penalty\": 0.0, \"presence_penalty\": 0.0, \"max_tokens\": 1000, \"model\": \"gpt-3.5-turbo\", \"stream\": false}'\n",
      "{\n",
      "  \"id\": \"chatcmpl-8xgfnrk85kcFgvD0lYKjrty2u6dUw\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1709236079,\n",
      "  \"model\": \"gpt-3.5-turbo-0125\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"User asks about USA president, AI says Joe Biden. User asks height, AI states 6 feet. User wants cm conversion, AI gives 243.84 cm. User asks Obama's height, AI says 6.5 feet.\"\n",
      "      },\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 214,\n",
      "    \"completion_tokens\": 48,\n",
      "    \"total_tokens\": 262\n",
      "  },\n",
      "  \"system_fingerprint\": \"fp_86156a94a0\"\n",
      "}\n",
      "\n",
      "OpenAI Completions called: 207\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "CONVERSATION :::: \n",
      "\n",
      " User asks about USA president, AI says Joe Biden. User asks height, AI states 6 feet. User wants cm conversion, AI gives 243.84 cm. User asks Obama's height, AI says 6.5 feet. \n",
      "-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "convo = sum_chat.handle_conversation(chat_history[8:10])\n",
    "print(\"\\n\\n---\\n\\nCONVERSATION :::: \\n\\n\", convo, \"\\n-----\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "d323a016-9760-40b7-9a7e-f0b47569986f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New_Message: User: Can you add 100 cms to my frist question person's height?\n",
      "AI: Sure, Joe Biden's new height is 182.88 cms (6 feet) + 100 cms = 282.88 cms or 9.3 feet. | Count: 53\n",
      "Conversation:User asks about USA president, AI says Joe Biden. User asks height, AI says 6 feet. User converts to cm, AI provides 243.84 cm. User asks Biden's party, AI answers Democratic. User inquires about Obama's height, AI states 6.5 feet.User: Can you add 100 cms to my frist question person's height?\n",
      "AI: Sure, Joe Biden's new height is 182.88 cms (6 feet) + 100 cms = 282.88 cms or 9.3 feet. | Count: 112 | Limit: 70\n",
      "\n",
      "+++++\n",
      " \n",
      "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary within provided length of tokens.\n",
      "\n",
      "EXAMPLE\n",
      "Current summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Why do you think artificial intelligence is a force for good?\n",
      "AI: Because artificial intelligence will help humans reach their full potential.\n",
      "\n",
      "New summary length:\n",
      "20\n",
      "\n",
      "New summary:\n",
      "Human queries AI on AI's view. AI deems AI beneficial, enabling human potential...\n",
      "END OF EXAMPLE\n",
      "\n",
      "Current summary:\n",
      "User asks about USA president, AI says Joe Biden. User asks height, AI says 6 feet. User converts to cm, AI provides 243.84 cm. User asks Biden's party, AI answers Democratic. User inquires about Obama's height, AI states 6.5 feet.\n",
      "\n",
      "New lines of conversation:\n",
      "User: Can you add 100 cms to my frist question person's height?\n",
      "AI: Sure, Joe Biden's new height is 182.88 cms (6 feet) + 100 cms = 282.88 cms or 9.3 feet.\n",
      "\n",
      "New summary length:\n",
      "50\n",
      "\n",
      "New summary:\n",
      " \n",
      "++++\n",
      "\n",
      "b'{\"messages\": [{\"role\": \"user\", \"content\": \"\\\\nProgressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary within provided length of tokens.\\\\n\\\\nEXAMPLE\\\\nCurrent summary:\\\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\\\\n\\\\nNew lines of conversation:\\\\nHuman: Why do you think artificial intelligence is a force for good?\\\\nAI: Because artificial intelligence will help humans reach their full potential.\\\\n\\\\nNew summary length:\\\\n20\\\\n\\\\nNew summary:\\\\nHuman queries AI on AI\\'s view. AI deems AI beneficial, enabling human potential...\\\\nEND OF EXAMPLE\\\\n\\\\nCurrent summary:\\\\nUser asks about USA president, AI says Joe Biden. User asks height, AI says 6 feet. User converts to cm, AI provides 243.84 cm. User asks Biden\\'s party, AI answers Democratic. User inquires about Obama\\'s height, AI states 6.5 feet.\\\\n\\\\nNew lines of conversation:\\\\nUser: Can you add 100 cms to my frist question person\\'s height?\\\\nAI: Sure, Joe Biden\\'s new height is 182.88 cms (6 feet) + 100 cms = 282.88 cms or 9.3 feet.\\\\n\\\\nNew summary length:\\\\n50\\\\n\\\\nNew summary:\\\\n\"}], \"temperature\": 0.0, \"top_p\": 1.0, \"frequency_penalty\": 0.0, \"presence_penalty\": 0.0, \"max_tokens\": 1000, \"model\": \"gpt-3.5-turbo\", \"stream\": false}'\n",
      "{\n",
      "  \"id\": \"chatcmpl-8xgVWSb3QsA57RYxya120di5xkvzc\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1709235442,\n",
      "  \"model\": \"gpt-3.5-turbo-0125\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"User asks about USA president, AI says Joe Biden. User asks height, AI says 6 feet. User converts to cm, AI provides 243.84 cm. User asks Biden's party, AI answers Democratic. User inquires about Obama's height, AI states 6.5 feet. User requests adding 100 cms to Biden's height. AI calculates Biden's new height as 282.88 cms or 9.3 feet.\"\n",
      "      },\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 252,\n",
      "    \"completion_tokens\": 90,\n",
      "    \"total_tokens\": 342\n",
      "  },\n",
      "  \"system_fingerprint\": \"fp_86156a94a0\"\n",
      "}\n",
      "\n",
      "OpenAI Completions called: 245\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "CONVERSATION :::: \n",
      "\n",
      " None \n",
      "-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "convo = sum_chat.handle_conversation(chat_history[10:12])\n",
    "print(\"\\n\\n---\\n\\nCONVERSATION :::: \\n\\n\", convo, \"\\n-----\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da1e56c-8ebf-4319-ab7f-f64c81439c5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e32f55b-25d7-498f-bd88-ba3bdefe2a21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5894b5-27e0-452e-976e-e1f3940e6363",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacda91f-49b0-4011-8fee-2b21e7d2f1a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b739863d-ce63-4c16-9dc1-198e56a34e27",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'prompt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msummarized_conversation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'prompt'"
     ]
    }
   ],
   "source": [
    "summarized_conversation.prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "27d9f098-c01b-4b15-abb3-3aee1db5638a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Hello! Who is the president of USA?\n",
      "AI: The president of USA is Joe Biden.\n",
      "\n",
      "\n",
      "User: What is height?\n",
      "AI: It's 6 feet.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(conversation_rolling_window.conversation_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82254d77-b3f1-4525-8398-627c6915ee4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17dc566-0cd5-46ad-8571-8dfab28c924d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc2e1ea-6071-42f3-a734-8ae2a3daf1c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7d8e05f3-d0f5-4e11-9224-10ac4753d879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'User: Hello! Who is the president of USA?\\nAI: The president of USA is Joe Biden.\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarized_conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25186260-d3fb-4f14-80c5-acbfd3064e07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214dc542-5c58-4a23-bd57-4351873aa878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224360b2-2f8e-454d-b4cd-f1617b5eafc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dea199-9793-4868-b902-97d954de34a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0431e140-dcc9-4312-b86a-1fab36261a45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85a6e2b-ee83-44c5-beb0-bf3a20c3f661",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886ba726-5f25-4b70-a1cb-ce74296221ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_3.11.3",
   "language": "python",
   "name": "python_3.11.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
